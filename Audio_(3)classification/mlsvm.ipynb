{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b3c336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished loading all machine data\n",
      "Total samples: 3807\n",
      "Train shape: (2664, 45)\n",
      "Test shape: (1143, 45)\n",
      "‚úÖ SVM Training Complete!\n",
      "\n",
      "Confusion Matrix:\n",
      "[[421 155]\n",
      " [204 363]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70       576\n",
      "           1       0.70      0.64      0.67       567\n",
      "\n",
      "    accuracy                           0.69      1143\n",
      "   macro avg       0.69      0.69      0.69      1143\n",
      "weighted avg       0.69      0.69      0.69      1143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ================================\n",
    "# Step 1: Setup\n",
    "# ================================\n",
    "DATASET_PATH = r\"D:\\\\\"\n",
    "MACHINE_TYPES = [\"fan\", \"pump\", \"gearbox\"]\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# ================================\n",
    "# Step 2: Advanced Feature Extraction\n",
    "# ================================\n",
    "def extract_advanced_features(file_path, n_mfcc=40):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
    "\n",
    "    # Spectral Centroid\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "\n",
    "    # Spectral Bandwidth\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "\n",
    "    # Zero Crossing Rate\n",
    "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "\n",
    "    # RMS Energy\n",
    "    rms = np.mean(librosa.feature.rms(y=y))\n",
    "\n",
    "    # Chroma STFT\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "    \n",
    "    # Combine features\n",
    "    return np.hstack((mfcc_scaled, spectral_centroid, spectral_bandwidth, zero_crossing_rate, rms, chroma_stft))\n",
    "\n",
    "# ================================\n",
    "# Step 3: Load Data from All 3 Machines\n",
    "# ================================\n",
    "for machine in MACHINE_TYPES:\n",
    "    for test_type in [\"source_test\", \"target_test\"]:\n",
    "        test_folder = os.path.join(DATASET_PATH, machine, test_type)\n",
    "        for root, dirs, files in os.walk(test_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    label = 1 if \"anomaly\" in file else 0\n",
    "                    features = extract_advanced_features(file_path)\n",
    "                    X_all.append(features)\n",
    "                    y_all.append(label)\n",
    "\n",
    "print(\"‚úÖ Finished loading all machine data\")\n",
    "print(\"Total samples:\", len(X_all))\n",
    "\n",
    "# ================================\n",
    "# Step 4: Prepare Features\n",
    "# ================================\n",
    "X_all = np.array(X_all)\n",
    "y_all = np.array(y_all)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_all, test_size=0.3, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "\n",
    "# ================================\n",
    "# Step 5: Train SVM Classifier\n",
    "# ================================\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ SVM Training Complete!\")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Evaluate\n",
    "# ================================\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3811fdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features extracted from all machines.\n",
      "Total samples: 3807\n",
      "Train shape: (2664, 45)\n",
      "Test shape: (1143, 45)\n",
      "üîç Starting Grid Search (this will take time)...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "‚úÖ Best Hyperparameters Found:\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[451 125]\n",
      " [152 415]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.77       576\n",
      "           1       0.77      0.73      0.75       567\n",
      "\n",
      "    accuracy                           0.76      1143\n",
      "   macro avg       0.76      0.76      0.76      1143\n",
      "weighted avg       0.76      0.76      0.76      1143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ================================\n",
    "# Step 1: Setup\n",
    "# ================================\n",
    "DATASET_PATH = r\"D:\\\\\"\n",
    "MACHINE_TYPES = [\"fan\", \"pump\", \"gearbox\"]\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# ================================\n",
    "# Step 2: Advanced Feature Extraction\n",
    "# ================================\n",
    "def extract_advanced_features(file_path, n_mfcc=40):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
    "\n",
    "    # Spectral Centroid\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "\n",
    "    # Spectral Bandwidth\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "\n",
    "    # Zero Crossing Rate\n",
    "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "\n",
    "    # RMS Energy\n",
    "    rms = np.mean(librosa.feature.rms(y=y))\n",
    "\n",
    "    # Chroma STFT\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "    \n",
    "    return np.hstack((mfcc_scaled, spectral_centroid, spectral_bandwidth, zero_crossing_rate, rms, chroma_stft))\n",
    "\n",
    "# ================================\n",
    "# Step 3: Load Data\n",
    "# ================================\n",
    "for machine in MACHINE_TYPES:\n",
    "    for test_type in [\"source_test\", \"target_test\"]:\n",
    "        folder = os.path.join(DATASET_PATH, machine, test_type)\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    path = os.path.join(root, file)\n",
    "                    label = 1 if \"anomaly\" in file else 0\n",
    "                    features = extract_advanced_features(path)\n",
    "                    X_all.append(features)\n",
    "                    y_all.append(label)\n",
    "\n",
    "print(\"‚úÖ Features extracted from all machines.\")\n",
    "print(\"Total samples:\", len(X_all))\n",
    "\n",
    "# ================================\n",
    "# Step 4: Prepare Data\n",
    "# ================================\n",
    "X = np.array(X_all)\n",
    "y = np.array(y_all)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "\n",
    "# ================================\n",
    "# Step 5: Grid Search for SVM\n",
    "# ================================\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 1, 0.1, 0.01],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "print(\"üîç Starting Grid Search (this will take time)...\")\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=3, verbose=2, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# ================================\n",
    "# Step 6: Best SVM Evaluation\n",
    "# ================================\n",
    "print(\"\\n‚úÖ Best Hyperparameters Found:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190590bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "# =========================\n",
    "# Step 1: Setup\n",
    "# =========================\n",
    "DATASET_PATH = r\"D:\\\\\"  # <-- Change this to your dataset location\n",
    "MACHINE_TYPES = [\"fan\", \"pump\", \"gearbox\"]\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# =========================\n",
    "# Step 2: Advanced Feature Extraction\n",
    "# =========================\n",
    "def extract_advanced_features(file_path, n_mfcc=40):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Feature blocks\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "\n",
    "    features = np.hstack([\n",
    "        np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
    "        np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "        np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "        np.mean(zcr), np.std(zcr),\n",
    "        np.mean(rms), np.std(rms),\n",
    "        np.mean(chroma_stft), np.std(chroma_stft),\n",
    "        np.mean(rolloff), np.std(rolloff),\n",
    "        np.mean(tonnetz), np.std(tonnetz)\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# =========================\n",
    "# Step 3: Load Data\n",
    "# =========================\n",
    "for machine in MACHINE_TYPES:\n",
    "    for test_type in [\"source_test\", \"target_test\"]:\n",
    "        folder = os.path.join(DATASET_PATH, machine, test_type)\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    path = os.path.join(root, file)\n",
    "                    label = 1 if \"anomaly\" in file else 0\n",
    "                    features = extract_advanced_features(path)\n",
    "                    X_all.append(features)\n",
    "                    y_all.append(label)\n",
    "\n",
    "print(\"‚úÖ All Features Extracted\")\n",
    "print(\"Total Samples:\", len(X_all))\n",
    "\n",
    "# =========================\n",
    "# Step 4: Prepare Dataset\n",
    "# =========================\n",
    "X = np.array(X_all)\n",
    "y = np.array(y_all)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train Shape:\", X_train.shape)\n",
    "print(\"Test Shape:\", X_test.shape)\n",
    "\n",
    "# =========================\n",
    "# Step 5: Class Weight Handling\n",
    "# =========================\n",
    "sample_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# =========================\n",
    "# Step 6: Model Training with Hyperparameter Tuning\n",
    "# =========================\n",
    "params = {\n",
    "    'max_depth': [6, 8],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'n_estimators': [300, 500],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=XGBClassifier(eval_metric='logloss', random_state=42, verbosity=0),\n",
    "    param_grid=params,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "print(\"üèÜ Best Params:\", grid.best_params_)\n",
    "\n",
    "# =========================\n",
    "# Step 7: Evaluation\n",
    "# =========================\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nüéØ Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nüî• Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nüî• Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# =========================\n",
    "# Step 8: Feature Importance Plot\n",
    "# =========================\n",
    "plot_importance(best_model, max_num_features=20, height=0.5)\n",
    "plt.title(\"Top 20 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# Step 9: Optional Ensemble Model (for extra boost)\n",
    "# =========================\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "     ('xgb', best_model),\n",
    "     ('rf', RandomForestClassifier(n_estimators=300, random_state=42)),\n",
    "     ('svc', SVC(probability=True, kernel='rbf'))\n",
    " ], voting='soft')\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_ensemble = ensemble.predict(X_test)\n",
    "\n",
    "print(\"\\nü§ñ Ensemble Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_ensemble) * 100))\n",
    "print(\"\\nüî• Ensemble Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_ensemble))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
