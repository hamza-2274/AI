{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02762dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 329ms/step - accuracy: 0.3416 - loss: 1.0980 - val_accuracy: 0.3630 - val_loss: 1.0915\n",
      "Epoch 2/20\n",
      "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 293ms/step - accuracy: 0.3655 - loss: 1.0823 - val_accuracy: 0.3963 - val_loss: 1.0665\n",
      "Epoch 3/20\n",
      "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 301ms/step - accuracy: 0.4640 - loss: 1.0171 - val_accuracy: 0.6371 - val_loss: 0.8289\n",
      "Epoch 4/20\n",
      "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 322ms/step - accuracy: 0.6885 - loss: 0.7284 - val_accuracy: 0.6952 - val_loss: 0.7101\n",
      "Epoch 5/20\n",
      "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 292ms/step - accuracy: 0.7630 - loss: 0.5826 - val_accuracy: 0.7038 - val_loss: 0.7118\n",
      "Epoch 6/20\n",
      "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 264ms/step - accuracy: 0.8138 - loss: 0.4822 - val_accuracy: 0.7084 - val_loss: 0.7044\n",
      "Epoch 7/20\n",
      "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.8511 - loss: 0.3990"
     ]
    }
   ],
   "source": [
    "# 1. IMPORTS\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 2. DOWNLOAD NLTK STUFF\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 3. LOAD DATA\n",
    "df = pd.read_csv('Balanced_dataset.csv')\n",
    "\n",
    "# 4. TEXT PREPROCESSING\n",
    "df['lowercased'] = df['Text'].str.lower()  # Convert text to lowercase\n",
    "df['clean'] = df['lowercased'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))  # Remove non-alphabetic characters\n",
    "df['token'] = df['clean'].apply(lambda x: x.split())  # Tokenize text into words\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['no_stopwords'] = df['token'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Join tokens back into a string\n",
    "df['final_text'] = df['lemmatized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 5. LABEL ENCODING (-1, 0, 1 → 0, 1, 2)\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['Score'])\n",
    "\n",
    "# 6. TOKENIZE TEXT\n",
    "texts = df['final_text'].values\n",
    "labels = df['encoded_label'].values\n",
    "\n",
    "# Tokenizer to vectorize text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "max_len = 100\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 7. SPLIT DATA INTO TRAINING AND TEST SETS\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8. BUILD ENHANCED LSTM CLASSIFICATION MODEL\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_len))\n",
    "\n",
    "# Add Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Bidirectional LSTM with more units\n",
    "\n",
    "# Add another LSTM layer\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer with 3 classes\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes (negative, neutral, positive)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=SparseCategoricalCrossentropy(),\n",
    "              optimizer=Adam(learning_rate=0.0005),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 9. TRAIN THE MODEL (Changed epochs to 20)\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 10. EVALUATE THE MODEL\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_.astype(str)))\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nEnhanced LSTM Classification Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 11. PLOT TRAINING HISTORY\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"Enhanced LSTM Classification Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
